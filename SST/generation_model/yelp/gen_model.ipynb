{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gen_model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMFvo7wWMjJYEf/BOCoXZdi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2pterons/twigfarm/blob/main/gen_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypSi_UR7Ksyq"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import sys\n",
        "import math\n",
        "from transformers import *\n",
        "\n",
        "sys.path.insert(0, \"/DATA/joosung/fairseq_master\")\n",
        "\n",
        "import json\n",
        "f = open('/home/tf-dev-01/workspace_sol/style-transfer/Stable-Style-Transformer/generation_model/yelp/gpt_yelp_vocab.json')\n",
        "token2num = json.load(f)\n",
        "\n",
        "num2token = {}\n",
        "for key, value in token2num.items():\n",
        "    num2token[value] = key\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class styletransfer(nn.Module):\n",
        "    def __init__(self, drop_rate=0, gpu = True):\n",
        "        super(styletransfer, self).__init__()\n",
        "        self.gpu = gpu\n",
        "        self.gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "        \n",
        "        \"\"\"hyper parameters\"\"\"\n",
        "        self.n_vocab = 50259\n",
        "        self.emb_dim = 256\n",
        "        self.nhead = 4\n",
        "        self.num_layers = 3\n",
        "        \n",
        "        \"\"\"idx & length\"\"\"\n",
        "        self.START_IDX = 50257\n",
        "        self.PAD_IDX = 50258\n",
        "        self.EOS_IDX = 50256\n",
        "        self.MAX_SENT_LEN = 10\n",
        "        \n",
        "        \"\"\"attribute matrix\"\"\"\n",
        "        ## one_hot encoding\n",
        "        self.att_num = 2\n",
        "        self.matrix_A = nn.Linear(self.att_num, self.emb_dim)\n",
        "        \n",
        "        \"\"\"word embedding\"\"\"\n",
        "        self.emb_matrix = nn.Embedding(self.n_vocab, self.emb_dim, self.PAD_IDX) # 50259x1024\n",
        "        \n",
        "        \"\"\"Position embedding\"\"\"\n",
        "        self.pos_encoder = PositionalEncoding(self.emb_dim)\n",
        "        \n",
        "        \"\"\"Encoder\"\"\"\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=self.emb_dim, nhead=self.nhead)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=self.num_layers)       \n",
        "        \n",
        "        \"\"\"Decoder\"\"\"                \n",
        "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=self.emb_dim, nhead=self.nhead)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=self.num_layers)\n",
        "        self.matrix_D = nn.Linear(self.emb_dim, self.n_vocab) # emb_dim -> n_vocab\n",
        "        \n",
        "        \"\"\"parameters\"\"\"        \n",
        "        self.enc_params = list(self.encoder_layer.parameters())+list(self.transformer_encoder.parameters())\n",
        "        self.dec_params = list(self.decoder_layer.parameters())+list(self.transformer_decoder.parameters())+list(self.matrix_D.parameters())\n",
        "        self.aed_params = list(self.emb_matrix.parameters())+self.enc_params+self.dec_params\n",
        "\n",
        "    \"\"\"Modeling\"\"\"\n",
        "    def encoder(self, enc_input):\n",
        "        \"\"\"\n",
        "        enc_input: (batch, enc_len)\n",
        "        \"\"\"\n",
        "        word_emb = self.emb_matrix(enc_input) # (batch, enc_len, emb_dim)\n",
        "        word_emb = word_emb.transpose(0, 1) # (enc_len, batch, emb_dim)\n",
        "        word_pos = self.pos_encoder(word_emb) # (enc_len, batch, emb_dim)\n",
        "        out_enc = self.transformer_encoder(word_pos) # (enc_len, batch, emb_dim)\n",
        "        \n",
        "        return out_enc\n",
        "        \n",
        "    def decoder(self, enc_out, dec_input, attribute):\n",
        "        \"\"\"\n",
        "        enc_out: (enc_len, batch, emb_dim)\n",
        "        dec_input: (batch, dec_len)\n",
        "        attributes: (batch, 2)\n",
        "        \"\"\"\n",
        "        att_emb = self.matrix_A(attribute).unsqueeze(0) # (1. batch, emb_dim)\n",
        "        \n",
        "        word_emb = self.emb_matrix(dec_input) # (batch, dec_len, emb_dim)\n",
        "        word_emb = word_emb.transpose(0, 1) # (dec_len, batch, emb_dim)\n",
        "        word_pos = self.pos_encoder(word_emb) # (dec_len, batch, emb_dim)    \n",
        "        \n",
        "        start_token = self.emb_matrix(torch.tensor(self.START_IDX).cuda()) # (emb_dim)\n",
        "        start_token = start_token.repeat(1, dec_input.shape[0], 1) # (1, batch, emb_dim)        \n",
        "        style_dec_input = torch.cat([att_emb, start_token, word_pos], 0) # (dec_len+2, batch, emb_dim) w/ [att], [start]\n",
        "        \n",
        "        tgt_mask = self.generate_square_subsequent_mask(style_dec_input.shape[0]).cuda() # (dec_len+2, dec_len+2)\n",
        "\n",
        "        dec_out = self.transformer_decoder(style_dec_input, enc_out, tgt_mask=tgt_mask) # (dec_len+2, batch, emb_dim)\n",
        "        vocab_out = self.matrix_D(dec_out) # (dec_len+2, batch, n_vocab)\n",
        "        return dec_out, vocab_out\n",
        "    \n",
        "    def generator(self, enc_out, gen_len, attribute):\n",
        "        \"\"\"\n",
        "        enc_out: (enc_len, batch, emb_dim)\n",
        "        attributes: (batch, 2)\n",
        "        gen_len: len(dec_in)+1\n",
        "        \"\"\"\n",
        "        # initialization because there are no first token\n",
        "        batch = enc_out.shape[1]\n",
        "        att_emb = self.matrix_A(attribute).unsqueeze(0) # (1. batch, emb_dim)\n",
        "        start_token = self.emb_matrix(torch.tensor(self.START_IDX).cuda()) # (emb_dim)\n",
        "        start_token = start_token.repeat(1, batch, 1) # (1, batch, emb_dim)        \n",
        "        gen_input = torch.cat([att_emb, start_token], 0) # (2, batch, emb_dim) w/ [att], [start]\n",
        "        \n",
        "        for i in range(gen_len):\n",
        "            tgt_mask = self.generate_square_subsequent_mask(gen_input.shape[0]).cuda() # (pre_gen_len, pre_gen_len)\n",
        "            dec_out = self.transformer_decoder(gen_input, enc_out, tgt_mask=tgt_mask) # (pre_gen_len, batch, emb_dim)\n",
        "            vocab_out = self.matrix_D(dec_out) # (pre_gen_len, batch, n_vocab)\n",
        "            \n",
        "            vocab_idx = vocab_out.argmax(2) # (pre_gen_len, batch)\n",
        "            vocab_idx = vocab_idx.transpose(0, 1) # (batch, pre_gen_len)\n",
        "            \n",
        "            new_word_emb = self.emb_matrix(vocab_idx) # (batch, pre_gen_len, emb_dim)\n",
        "            new_word_emb = new_word_emb.transpose(0, 1) # (pre_gen_len, batch, emb_dim)\n",
        "#             gen_emb = torch.bmm(vocab_out, self.emb_matrix.weight.repeat(vocab_out.shape[0],1,1))\n",
        "            \n",
        "#             word_pos = self.pos_encoder(word_emb) # (enc_len, batch, emb_dim)\n",
        "            gen_input = torch.cat([gen_input, new_word_emb[-1:,:,:]]) # (pre_gen_len+1, batch, word_dim), pre_gen_len+=1        \n",
        "        \n",
        "        return vocab_out # (gen_len+2, batch, n_vocab)\n",
        "\n",
        "    def generate_square_subsequent_mask(self,sz): # len(sz)\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "    \n",
        "    \"\"\"calculation loss\"\"\"\n",
        "    def recon_loss(self, dec_input, vocab_out):\n",
        "        \"\"\"\n",
        "        dec_input: (batch, dec_len)\n",
        "        vocab_out: (dec_len+2, batch, n_vocab) with [att], [start]\n",
        "        \"\"\"\n",
        "        end_token = torch.tensor(self.EOS_IDX).cuda() # (1)\n",
        "        end_token = end_token.repeat(dec_input.shape[0], 1) # (batch, 1)\n",
        "        target_tokens = torch.cat([dec_input, end_token], 1) # (batch, dec_len+1) w/ [EOS]\n",
        "        \n",
        "        pred_out = vocab_out[1:,:,:] # (dec_len+1, batch, n_vocab)\n",
        "        pred_out = pred_out.permute(1,0,2) # (batch, dec_len+1, n_vocab)\n",
        "                \n",
        "        target_tokens = target_tokens.contiguous() # (batch, dec_len+1)\n",
        "        pred_out = pred_out.contiguous() # (batch, dec_len+1, n_vocab)\n",
        "    \n",
        "        target_tokens = target_tokens.view(-1) # (batch*(dec_len+1))\n",
        "        pred_out = pred_out.view(-1, pred_out.shape[2]) # (batch*(seq_len+1), n_vocab)\n",
        "        \n",
        "        recon_loss = F.cross_entropy(pred_out, target_tokens)                \n",
        "        \n",
        "        return recon_loss\n",
        "    \n",
        "    def cls_loss(self, attributes, cls_out):\n",
        "        \"\"\"\n",
        "        attributes: [0,1] or [1,0]\n",
        "        cls_out: (batch, 2) (logits)\n",
        "        \"\"\"        \n",
        "        targets = attributes.argmax(1) # (batch)\n",
        "        cls_loss = F.cross_entropy(cls_out, targets)\n",
        "        \n",
        "        if self.gpu == True:       \n",
        "            return cls_loss.cuda()\n",
        "        else:\n",
        "            return cls_loss\n",
        "        \n",
        "    \"\"\"inferenece\"\"\"\n",
        "    def dec2sen(self, vocab_out):\n",
        "        \"\"\"\n",
        "        vocab_out: (dec_len+2, batch, n_vocab) with att, start\n",
        "        \"\"\"\n",
        "        pred_out = vocab_out[1:,:,:] # (dec_len+1, batch, n_vocab) with [END]\n",
        "        pred_idx = torch.argmax(pred_out, 2) # (dec_len+1, batch)\n",
        "        pred_idx = pred_idx.squeeze(1) # (dec_len+1) because of batch=1\n",
        "        \n",
        "        token_list = []\n",
        "        dec_sen =''\n",
        "        for i in range(len(pred_idx)):\n",
        "            token = num2token[pred_idx[i].cpu().numpy().item()]\n",
        "            token_list.append(token)\n",
        "            \n",
        "            if 'Ġ' in token:\n",
        "                token = token.strip('Ġ')\n",
        "                dec_sen += ' '\n",
        "                dec_sen += token\n",
        "            else:\n",
        "                dec_sen += token\n",
        "        dec_sen = dec_sen.strip()\n",
        "            \n",
        "        \n",
        "        return token_list, dec_sen\n",
        "    \n",
        "    def generated_sentence(self, enc_out, attribute, ori_length):\n",
        "        \"\"\"\n",
        "        enc_out: (enc_len, batch, emb_dim)\n",
        "        dec_input: (batch, dec_len)\n",
        "        attributes: (batch, 2)\n",
        "        \"\"\"\n",
        "        batch = enc_out.shape[1]\n",
        "#         max_len = enc_out.shape[0]+3\n",
        "        max_len = ori_length+5\n",
        "        \n",
        "        # initialization because there are no first token\n",
        "        att_emb = self.matrix_A(attribute).unsqueeze(0) # (1. batch, emb_dim)\n",
        "        start_token = self.emb_matrix(torch.tensor(self.START_IDX).cuda()) # (emb_dim)\n",
        "        start_token = start_token.repeat(1, batch, 1) # (1, batch, emb_dim)        \n",
        "        gen_input = torch.cat([att_emb, start_token], 0) # (2, batch, emb_dim) w/ [att], [start]\n",
        "\n",
        "        tgt_mask = self.generate_square_subsequent_mask(gen_input.shape[0]).cuda() # (2, 2)        \n",
        "        \n",
        "        dec_out = self.transformer_decoder(gen_input, enc_out, tgt_mask=tgt_mask) # (2, batch, emb_dim)\n",
        "        vocab_out = self.matrix_D(dec_out) # (2, batch, n_vocab)\n",
        "        _, dec_sen = self.dec2sen(vocab_out)\n",
        "        \n",
        "        gen_vocab_out = []\n",
        "        for i in range(max_len):            \n",
        "            token_idx = torch.tensor(self.gpt_tokenizer.encode(dec_sen)).unsqueeze(0).cuda() # (batch, gen_len)\n",
        "            if self.EOS_IDX in token_idx:\n",
        "                break\n",
        "                \n",
        "            dec_out, vocab_out = self.decoder(enc_out, token_idx, attribute) # (dec_len+2, batch, emb_dim), (dec_len+2, batch, n_vocab)\n",
        "            dec_tokens, dec_sen = self.dec2sen(vocab_out)           \n",
        "            \n",
        "        return dec_sen\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}